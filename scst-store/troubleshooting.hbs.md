# Troubleshooting 

This topic contains troubleshooting and known issues for **Supply Chain Security Tools - Store**.

## Persistent volume retains data

### Symptom

If **Supply Chain Security Tools - Store** is deployed, deleted, redeployed, and the database password is changed during the redeployment, the
`metadata-store-db` pod fails to start.
This is caused by the persistent volume used by postgres retaining old data, even though the retention
policy is set to `DELETE`.

### <a id='persistent-volume-retains-data-solution'></a>Solution

>**Warning:** Changing the database password deletes your **Supply Chain Security Tools - Store** data.

To redeploy the app, either use the same database password or follow the steps below to erase the
data on the volume:

1. Deploy metadata-store app by using `kapp`.
1. Verify that the `metadata-store-db-*` pod fails.
1. Run:

    ```console
    kubectl exec -it metadata-store-db-<some-id> -n metadata-store /bin/bash
    ```

    Where `<some-id>` is the ID generated by Kubernetes and appended to the pod name.

1. Run `rm -rf /var/lib/postgresql/data/*` to delete all database data.

    Where `/var/lib/postgresql/data/*` is the path found in `postgres-db-deployment.yaml`.

1. Delete the `metadata-store` app by using `kapp`.
1. Deploy the `metadata-store` app by using `kapp`.

## Missing persistent volume

### Symptom

After Store is deployed, `metadata-store-db` pod might fail for missing volume while
`postgres-db-pv-claim` pvc is in `PENDING` state.
This is because the cluster where Store is deployed does not have `storageclass` defined.
`storageclass`'s provisioner is responsible for creating the persistent volume after
`metadata-store-db` attaches `postgres-db-pv-claim`.

### <a id='missing-persistent-volume-solution'></a>Solution

1. Verify that your cluster has `storageclass` by running `kubectl get storageclass`.
1. Create a `storageclass` in your cluster before deploying Store. For example:

    ```console
    # This is the storageclass that Kind uses
    kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

    # set the storage class as default
    kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    ```

## <a id='multicluster-store'></a> Multicluster Support: Error sending results to SCST - Store running in a different cluster

### Symptom

The [Store Ingress and multicluster support](scst-store/ingress-multicluster.md)
document instructs you on how to create `SecretExports` to share secrets for
communicating with the Store.
During installation, Supply Chain Security Tools - Scan (Scan) creates the
`SecretImport` for ingesting the TLS CA certificate secret,
but misses the `SecretImport` for the RBAC Auth token.

### Solution

As a workaround, apply the following YAML to the cluster running Scan and then
perform a rolling restart:

>**Note:** In some cases, you must update the namespaces before performing the rolling start.

```yaml
---
apiVersion: secretgen.carvel.dev/v1alpha1
kind: SecretImport
metadata:
  name: store-auth-token
  namespace: scan-link-system
spec:
  fromNamespace: metadata-store-secrets
```

The `Secret` for the RBAC Auth token is created and the scan can be re-run.
A rolling restart includes running the following:

```console
kubectl rollout restart deployment.apps/scan-link-controller-manager -n scan-link-system
```